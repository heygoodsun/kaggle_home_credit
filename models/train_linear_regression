{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"},{"sourceId":166996856,"sourceType":"kernelVersion"}],"dockerImageVersionId":30664,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import polars as pl\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.decomposition import TruncatedSVD\nimport dill\nimport gc\nimport time\n\n\ncurrent_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\nprint(\"this notebook training time is \", current_time)","metadata":{"_uuid":"94e04a98-d6e4-4f81-8c77-4dc2a1aac68c","_cell_guid":"d028a48b-4f7f-45fe-8362-ea86d716e0a2","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config():\n    seed=2024\n    num_folds=10\n    TARGET_NAME ='target'\n    batch_size=1000\n    \nimport random\n\ndef seed_everything(seed):\n    np.random.seed(seed)\n    random.seed(seed)\nseed_everything(Config.seed)\n\ncolname2dtype=pd.read_csv(\"/kaggle/input/home-credit-inconsistent-data-types/colname2dtype.csv\")\ncolname=colname2dtype['Column'].values\ndtype=colname2dtype['DataType'].values\n\ndtype2pl={}\ndtype2pl['Int64']=pl.Int64\ndtype2pl['Float64']=pl.Float64\ndtype2pl['String']=pl.String\ndtype2pl['Boolean']=pl.String\n\ncolname2dtype={}\nfor idx in range(len(colname)):\n    colname2dtype[colname[idx]]=dtype2pl[dtype[idx]]","metadata":{"_uuid":"8bf5b763-b33c-4229-8785-0bab1b71d6f0","_cell_guid":"ab7fc3eb-4570-4cb3-b29b-88a885c4d2a3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_df_null_col(df,margin=0.975):\n    cols=[]\n    for col in df.columns:\n        if df[col].isna().mean()>margin:\n            cols.append(col)\n    return cols\n\ndef find_last_case_id(df,id='case_id'):\n    df_copy=df.clone()\n    df_tail=df.tail(1)\n    \n    df_copy=df_copy.with_columns(pl.col(id).shift(-1).alias(f\"{id}_shift_-1\"))\n    df_last=df_copy.filter(pl.col(id)-pl.col(f'{id}_shift_-1')!=0).drop(f'{id}_shift_-1')\n    \n    df_last=pl.concat([df_last,df_tail])\n    \n    del df_copy,df_tail\n    gc.collect()\n    return df_last\n\ndef df_fillna(df,col,method=None):\n    if method ==None:\n        pass\n    if method == \"forward\":\n        df = df.select([pl.col(col).fill_null('forward')])\n    else:\n        df=df.with_columns(pl.col(col).fill_null(method).alias(col))\n    return df\n\ndef one_hot_encoder(df,col,unique):\n    \n    if len(unique)==2:\n        df=df.with_columns((pl.col(col)==unique[0]).cast(pl.Int8).alias(f\"{col}_{unique[0]}\"))\n    else:\n        for idx in range(len(unique)):\n            df=df.with_columns((pl.col(col)==unique[idx]).cast(pl.Int8).alias(f\"{col}_{unique[idx]}\"))\n    return df.drop(col)\n\ndef last_features_merge(feats,last_df,last_features=[]):\n    \n    last_df=last_df.select(['case_id']+[last[0] for last in last_features])\n    \n    for last in last_features:\n        col,fill=last\n        last_df=df_fillna(last_df,col,method=fill)\n    \n    feats=feats.join(last_df,on='case_id',how='left')\n    return feats\n\ndef group_features_merge(feats,group_df,group_features=[],group_name='applprev2'):\n    \n    group_df=group_df.select(['case_id']+[g[0] for g in group_features])\n    \n    for group in group_features:\n        if group_df[group[0]].dtype==pl.String:\n            col,fill,one_hot=group\n            group_df=df_fillna(group_df,col,method=fill)\n            if one_hot==None:\n                group_df=group_df.drop(col) \n            else:\n                group_df=one_hot_encoder(group_df,col,one_hot)\n                for value in one_hot:\n                    new_col=f\"{col}_{value}\"\n                    feat=feat=group_df.group_by('case_id').agg( \n                                               pl.mean(new_col).alias(f\"mean_{group_name}_{new_col}\"),\n                                               pl.std(new_col).alias(f\"std_{group_name}_{new_col}\"),\n                                               pl.count(new_col).alias(f\"count_{group_name}_{new_col}\"),\n                                             )\n                    feats=feats.join(feat,on='case_id',how='left')\n        else:\n            col,fill=group\n            group_df=df_fillna(group_df,col,method=fill)\n            feat=group_df.group_by('case_id').agg( pl.max(col).alias(f\"max_{group_name}_{col}\"),\n                                   pl.mean(col).alias(f\"mean_{group_name}_{col}\"),\n                                   pl.median(col).alias(f\"median_{group_name}_{col}\"),\n                                   pl.std(col).alias(f\"std_{group_name}_{col}\"),\n                                   pl.min(col).alias(f\"min_{group_name}_{col}\"),\n                                   pl.count(col).alias(f\"count_{group_name}_{col}\"),\n                                   pl.sum(col).alias(f\"sum_{group_name}_{col}\"),\n                                   pl.n_unique(col).alias(f\"n_unique_{group_name}_{col}\"),\n                                   pl.first(col).alias(f\"first_{group_name}_{col}\"),\n                                   pl.last(col).alias(f\"last_{group_name}_{col}\")\n                                 )\n            feats=feats.join(feat,on='case_id',how='left')\n    return feats\n\ndef set_table_dtypes(df):\n    for col in df.columns:\n        df=df.with_columns(pl.col(col).cast(colname2dtype[col]).alias(col))\n    return df\n\ndef preprocessor(mode='train'):\n    print(f\"{mode} base file after break.number 1\")\n    feats=pl.read_csv(f\"/kaggle/input/home-credit-credit-risk-model-stability/csv_files/{mode}/{mode}_base.csv\").pipe(set_table_dtypes)\n    feats=feats.drop(['date_decision','MONTH','WEEK_NUM'])\n    print(\"-\"*30)\n    \n    print(f\"{mode} applprev_2 file after break. number:1\")\n    applprev2=pl.read_csv(f\"/kaggle/input/home-credit-credit-risk-model-stability/csv_files/{mode}/{mode}_applprev_2.csv\").pipe(set_table_dtypes)\n    applprev2=applprev2.with_columns(\n                \n               ( (pl.col('cacccardblochreas_147M')!=pl.col('cacccardblochreas_147M'))&(pl.col('conts_type_509L')!=pl.col('conts_type_509L')) )\\\n                .alias(\"no_credit\")\n                )\n    applprev2=applprev2.with_columns(\n                \n                ( (pl.col('cacccardblochreas_147M')!=pl.col('cacccardblochreas_147M'))&(pl.col('conts_type_509L')==pl.col('conts_type_509L'))) \\\n                .alias(\"no_frozen_credit\").cast(pl.Int8)\n                )\n    applprev2=applprev2.with_columns(\n                \n                (pl.col('cacccardblochreas_147M')==pl.col('cacccardblochreas_147M'))\\\n                .alias(\"frozen_credit\").cast(pl.Int8)\n                )\n    \n    applprev2_last=find_last_case_id(applprev2)\n    \n    last_features=[['conts_type_509L','WHATSAPP'],\n                   ['no_credit',0],\n                   ['no_frozen_credit',0],\n                   ['frozen_credit',0]\n                  ]\n    feats=last_features_merge(feats,applprev2_last,last_features)\n    \n    group_features=[['cacccardblochreas_147M','a55475b1',\\\n                     [\"P19_60_110\",\"P17_56_144\",\"a55475b1\",\"P201_63_60\",\"P127_74_114\",\"P133_119_56\",\"P41_107_150\",\"P23_105_103\"\"P33_145_161\"]],\n                    ['credacc_cards_status_52L','UNCONFIRMED',\\\n                     ['BLOCKED','UNCONFIRMED','RENEWED', 'CANCELLED', 'INACTIVE', 'ACTIVE']],\n                     ['num_group1',0],\n                   ['num_group2',0],\n                   ]\n    feats=group_features_merge(feats,applprev2,group_features,group_name='applprev2')\n    del applprev2,applprev2_last\n    gc.collect()\n    print(\"-\"*30)\n    \n    print(\"credit bureau b num 2\")\n    bureau_b_1=pl.read_csv(f\"/kaggle/input/home-credit-credit-risk-model-stability/csv_files/{mode}/{mode}_credit_bureau_b_1.csv\").pipe(set_table_dtypes)\n    bureau_b_2=pl.read_csv(f\"/kaggle/input/home-credit-credit-risk-model-stability/csv_files/{mode}/{mode}_credit_bureau_b_2.csv\").pipe(set_table_dtypes)\n    bureau_b_1_last=find_last_case_id(bureau_b_1,id='case_id')\n    bureau_b_2_last=find_last_case_id(bureau_b_2,id='case_id')\n    feats=feats.join(bureau_b_1_last,on='case_id',how='left')\n    feats=feats.join(bureau_b_2_last,on='case_id',how='left')\n\n    del bureau_b_1,bureau_b_1_last,bureau_b_2,bureau_b_2_last\n    gc.collect()\n\n    print(f\"{mode} debitcard file after break num 1\")\n    debitcard=pl.read_csv(f\"/kaggle/input/home-credit-credit-risk-model-stability/csv_files/{mode}/{mode}_debitcard_1.csv\").pipe(set_table_dtypes)\n    debitcard_last=find_last_case_id(debitcard,id='case_id')\n    \n    last_features=[['last180dayaveragebalance_704A',0],\n                   ['last180dayturnover_1134A',30000],\n                   ['last30dayturnover_651A',0]\n                  ]\n    feats=last_features_merge(feats,debitcard_last,last_features)\n    group_features=[['num_group1',0]\n                  ]\n    feats=group_features_merge(feats,debitcard,group_features,group_name='debitcard')\n    del debitcard,debitcard_last\n    gc.collect()\n\n    print(f\"{mode} deposit file num 1\")\n    deposit=pl.read_csv(f\"/kaggle/input/home-credit-credit-risk-model-stability/csv_files/{mode}/{mode}_deposit_1.csv\").pipe(set_table_dtypes)\n    \n    for idx in range(1,len(deposit.columns)):\n        col=deposit.columns[idx]\n        column_type = deposit[col].dtype\n        is_numeric = (column_type == pl.datatypes.Int64) or (column_type == pl.datatypes.Float64) \n        if is_numeric:\n            feat=deposit.group_by('case_id').agg( pl.max(col).alias(f\"max_deposit_{col}\"),\n                                           pl.mean(col).alias(f\"mean_deposit_{col}\"),\n                                           pl.median(col).alias(f\"median_deposit_{col}\"),\n                                           pl.std(col).alias(f\"std_deposit_{col}\"),\n                                           pl.min(col).alias(f\"min_deposit_{col}\"),\n                                           pl.count(col).alias(f\"count_deposit_{col}\"),\n                                           pl.sum(col).alias(f\"sum_deposit_{col}\"),\n                                           pl.n_unique(col).alias(f\"n_unique_deposit_{col}\"),\n                                           pl.first(col).alias(f\"first_deposit_{col}\"),\n                                           pl.last(col).alias(f\"last_deposit_{col}\")\n                                         )\n            feats=feats.join(feat,on='case_id',how='left')\n    del deposit\n    gc.collect()\n    \n    print(f\"{mode} other file after break number 1\")\n    other=pl.read_csv(f\"/kaggle/input/home-credit-credit-risk-model-stability/csv_files/{mode}/{mode}_other_1.csv\").pipe(set_table_dtypes)\n    other_last=find_last_case_id(other)\n    \n    \n    last_features=[['amtdepositbalance_4809441A',0]\n                  ]\n    feats=last_features_merge(feats,other_last,last_features)\n\n    group_features=[['amtdebitincoming_4809443A',0],\n                     ['amtdebitoutgoing_4809440A',0],\n                     ['amtdepositincoming_4809444A',0], \n                     ['amtdepositoutgoing_4809442A',0]\n                   ]\n    feats=group_features_merge(feats,other,group_features,group_name='other')\n    \n    del other,other_last\n    gc.collect()\n    \n\n    print(\"person 1 num 1\")\n    person1=pl.read_csv(f\"/kaggle/input/home-credit-credit-risk-model-stability/csv_files/{mode}/{mode}_person_1.csv\").pipe(set_table_dtypes)\n    \n    person1=person1.drop(['birthdate_87D','childnum_185L','gender_992L','housingtype_772L','isreference_387L','maritalst_703L','role_993L'])                   \n    \n    person1=person1.select(['case_id','contaddr_matchlist_1032L','contaddr_smempladdr_334L','empl_employedtotal_800L','language1_981M',\n                           'persontype_1072L','persontype_792L','remitter_829L','role_1084L','safeguarantyflag_411L','sex_738L'])\n    person1_last=find_last_case_id(person1)\n    feats=feats.join(person1_last,on='case_id',how='left')\n    \n    del person1,person1_last\n    gc.collect()\n    \n\n    print(f\"{mode} person2 file after break number 1\")\n    \n    person2=pl.read_csv(f\"/kaggle/input/home-credit-credit-risk-model-stability/csv_files/{mode}/{mode}_person_2.csv\").pipe(set_table_dtypes)\n    \n    person2=person2.drop(['addres_role_871L','empls_employedfrom_796D','relatedpersons_role_762T'])\n    \n    person2=person2.drop(['addres_district_368M','addres_zip_823M','empls_employer_name_740M'])\n    \n    group_features=[['conts_role_79M','a55475b1',\n                     ['a55475b1', 'P38_92_157', 'P7_147_157', 'P177_137_98', 'P125_14_176', \n                      'P125_105_50', 'P115_147_77', 'P58_79_51','P124_137_181', 'P206_38_166', 'P42_134_91']\n                    ],\n                    ['empls_economicalst_849M','a55475b1',\n                    ['a55475b1', 'P164_110_33', 'P22_131_138', 'P28_32_178','P148_57_109', 'P7_47_145', 'P164_122_65', 'P112_86_147','P82_144_169', 'P191_80_124']\n                    ],\n                    ['num_group1',0],\n                   ['num_group2',0],\n                   ]\n    del person2\n    gc.collect()\n    \n    print(f\"static_0 file num 2(3)\")\n    \n    static_0_0=pl.read_csv(f\"/kaggle/input/home-credit-credit-risk-model-stability/csv_files/{mode}/{mode}_static_0_0.csv\").pipe(set_table_dtypes)\n    static_0_1=pl.read_csv(f\"/kaggle/input/home-credit-credit-risk-model-stability/csv_files/{mode}/{mode}_static_0_1.csv\").pipe(set_table_dtypes)\n    \n    static=pl.concat([static_0_0,static_0_1],how=\"vertical_relaxed\")\n    if mode=='test':\n        static_0_2=pl.read_csv(f\"/kaggle/input/home-credit-credit-risk-model-stability/csv_files/{mode}/{mode}_static_0_2.csv\").pipe(set_table_dtypes)\n        static=pl.concat([static,static_0_2],how=\"vertical_relaxed\")\n    feats=feats.join(static,on='case_id',how='left')\n    del static,static_0_0,static_0_1\n    gc.collect()\n    \n    print(f\"{mode} static_cb_file after break num 1\")\n    static_cb=pl.read_csv(f\"/kaggle/input/home-credit-credit-risk-model-stability/csv_files/{mode}/{mode}_static_cb_0.csv\").pipe(set_table_dtypes)\n    \n    static_cb=static_cb.drop(['assignmentdate_4955616D', 'dateofbirth_342D','for3years_128L',\n                            'for3years_504L','for3years_584L','formonth_118L','formonth_206L','formonth_535L',\n                           'forquarter_1017L', 'forquarter_462L','forquarter_634L','fortoday_1092L',\n                           'forweek_1077L','forweek_528L','forweek_601L','foryear_618L','foryear_818L','foryear_850L','pmtaverage_4955615A','pmtcount_4955617L','riskassesment_302T','riskassesment_940T'])\n    static_cb=static_cb.drop(['birthdate_574D','dateofbirth_337D',\n                             'assignmentdate_238D','assignmentdate_4527235D',\n                              'responsedate_1012D','responsedate_4527233D','responsedate_4917613D',\n                             ])\n    \n    \n    last_features=[ ['contractssum_5085716L',0],\n                    ['days120_123L',0],\n                    ['days180_256L',0],\n                    ['days30_165L',0],\n                    ['days360_512L',1],\n                    ['days90_310L',0],\n                    ['description_5085714M','a55475b1'],\n                    \n                    ['education_88M','a55475b1'],\n                    ['firstquarter_103L',0],\n                    ['secondquarter_766L',0],\n                    ['thirdquarter_1082L',0],\n                    ['fourthquarter_440L',0],\n                    ['maritalst_385M','a55475b1'],\n                    \n                    ['numberofqueries_373L',1],\n                    ['pmtaverage_3A',0],\n                    \n                    \n                    ['pmtcount_693L', 6],\n                    ['pmtscount_423L',6.0],\n                    ['pmtssum_45A',0],\n                    ['requesttype_4525192L','DEDUCTION_6'],\n                  ]\n    feats=last_features_merge(feats,static_cb,last_features)\n    \n    feats=feats.with_columns( (pl.col('days180_256L')-pl.col('days120_123L')).alias(\"daysgap60\"))\n    feats=feats.with_columns( (pl.col('days180_256L')-pl.col('days30_165L')).alias(\"daysgap150\"))\n    feats=feats.with_columns( (pl.col('days120_123L')-pl.col('days30_165L')).alias(\"daysgap90\"))\n    \n    feats=feats.with_columns( (pl.col('firstquarter_103L')+pl.col('secondquarter_766L')+pl.col('thirdquarter_1082L')+pl.col('fourthquarter_440L')).alias(\"totalyear_result\"))\n    \n    del static_cb\n    gc.collect()\n    print(\"-\"*30)\n    \n    print(f\"{mode} tax_a file after break num 1\")\n    tax_a=pl.read_csv(f\"/kaggle/input/home-credit-credit-risk-model-stability/csv_files/{mode}/{mode}_tax_registry_a_1.csv\").pipe(set_table_dtypes)\n    \n    group_features=[['amount_4527230A',850],\n                     ['num_group1',0]\n                   ]\n    feats=group_features_merge(feats,tax_a,group_features,group_name='tax_a')\n    del tax_a\n    gc.collect()\n    print(\"-\"*30)\n    \n    print(f\"{mode} tax_b file after break num 1\")\n    tax_b=pl.read_csv(f\"/kaggle/input/home-credit-credit-risk-model-stability/csv_files/{mode}/{mode}_tax_registry_b_1.csv\").pipe(set_table_dtypes)\n    \n    group_features=[['amount_4917619A',6885],\n                    ['num_group1',0]\n                  ]\n    feats=group_features_merge(feats,tax_b,group_features,group_name='tax_b')\n    del tax_b\n    gc.collect()\n    print(\"-\"*30)\n    \n    print(f\"{mode} tax_c file after break num 1\")\n    tax_c=pl.read_csv(f\"/kaggle/input/home-credit-credit-risk-model-stability/csv_files/{mode}/{mode}_tax_registry_c_1.csv\").pipe(set_table_dtypes)\n    if len(tax_c)==0:\n        tax_c=pl.read_csv(f\"/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_tax_registry_c_1.csv\").pipe(set_table_dtypes)\n        \n    \n    tax_c=tax_c.drop(['employername_160M','processingdate_168D'])\n    \n    group_features=[['pmtamount_36A',850],\n                    ['num_group1',0]\n                  ]\n    feats=group_features_merge(feats,tax_c,group_features,group_name='tax_c')\n    del tax_c\n    gc.collect()\n    print(\"-\"*30)\n    \n    return feats\ntrain_feats=preprocessor(mode='train')\ntest_feats=preprocessor(mode='test')\n\ntrain_feats=train_feats.to_pandas()\ntest_feats=test_feats.to_pandas()\n\n\nmode_values = train_feats.mode().iloc[0]\n\ntrain_feats = train_feats.fillna(mode_values)\n\ntest_feats = test_feats.fillna(mode_values)","metadata":{"_uuid":"30d1460a-a4d2-4d2e-b663-42e874de84a9","_cell_guid":"0c4c4bac-a2cf-4082-a394-4879516fecd7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"----------string one hot encoder ****\")\nfor col in test_feats.columns:\n    n_unique=train_feats[col].nunique()\n    \n    \n    if n_unique==2 and train_feats[col].dtype=='object':\n        print(f\"one_hot_2:{col}\")\n        unique=train_feats[col].unique()\n        \n        train_feats[col]=(train_feats[col]==unique[0]).astype(int)\n        test_feats[col]=(test_feats[col]==unique[0]).astype(int)\n    elif (n_unique<10) and train_feats[col].dtype=='object':\n        print(f\"one_hot_10:{col}\")\n        unique=train_feats[col].unique()\n        for idx in range(len(unique)):\n            if unique[idx]==unique[idx]:\n                train_feats[col+\"_\"+str(idx)]=(train_feats[col]==unique[idx]).astype(int)\n                test_feats[col+\"_\"+str(idx)]=(test_feats[col]==unique[idx]).astype(int)\n        train_feats.drop([col],axis=1,inplace=True)\n        test_feats.drop([col],axis=1,inplace=True)\nprint(\"----------drop other string or unique value or full null value ****\")\ndrop_cols=[]\nfor col in test_feats.columns:\n    if (train_feats[col].dtype=='object') or (test_feats[col].dtype=='object') \\\n        or (train_feats[col].nunique()==1) or train_feats[col].isna().mean()>0.99:\n        drop_cols+=[col]\n\ndrop_cols+=['case_id']\ntrain_feats.drop(drop_cols,axis=1,inplace=True)\ntest_feats.drop(drop_cols,axis=1,inplace=True)\nprint(f\"len(train_feats):{len(train_feats)},total_features_counts:{len(test_feats.columns)}\")\ntrain_feats.head()","metadata":{"_uuid":"0f2f79d2-c484-4b7d-ae78-e0d5a9d59044","_cell_guid":"af2b2e6c-a838-44a1-b682-846f2977ee42","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, float16_as32=True):\n    \n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        if col_type != object:\n            c_min,c_max = df[col].min(),df[col].max() \n            if str(col_type)[:3] == 'int':\n                \n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                \n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                \n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                \n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                \n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    if float16_as32:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float16)  \n                \n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                \n                else:\n                    df[col] = df[col].astype(np.float64)\n    \n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    \n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\ntrain_feats = reduce_mem_usage(train_feats)\ntest_feats = reduce_mem_usage(test_feats)","metadata":{"_uuid":"ef60310e-0a25-43d4-b2f9-11c88716e124","_cell_guid":"0a6ab88c-4858-4e32-96c0-8cb2654c3e32","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pearson_corr(x1,x2):\n    \"\"\"\n    x1,x2:np.array\n    \"\"\"\n    mean_x1=np.mean(x1)\n    mean_x2=np.mean(x2)\n    std_x1=np.std(x1)\n    std_x2=np.std(x2)\n    pearson=np.mean((x1-mean_x1)*(x2-mean_x2))/(std_x1*std_x2)\n    return pearson\n\nchoose_cols=[]\nfor col in train_feats.columns:\n    if col!='target':\n        pearson=pearson_corr(train_feats[col].values,train_feats['target'].values) \n        if abs(pearson)>0.0025:\n            choose_cols.append(col)\nprint(f\"len(choose_cols):{len(choose_cols)},choose_cols:{choose_cols}\")","metadata":{"_uuid":"a4b1e8d9-eb6a-4995-9277-199c521074f2","_cell_guid":"eb136074-b94a-48a7-abdc-0f11aca4675f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pickle_dump(obj, path):\n    with open(path, mode=\"wb\") as f:\n        dill.dump(obj, f, protocol=4)","metadata":{"_uuid":"7d524123-ee49-4782-924e-a178ba32e369","_cell_guid":"42838091-24ea-42f7-a006-5d7906e2b8b1","collapsed":false,"execution":{"iopub.status.busy":"2024-05-24T13:08:59.056669Z","iopub.execute_input":"2024-05-24T13:08:59.057038Z","iopub.status.idle":"2024-05-24T13:08:59.093842Z","shell.execute_reply.started":"2024-05-24T13:08:59.057007Z","shell.execute_reply":"2024-05-24T13:08:59.092926Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nX=train_feats[choose_cols].copy()\ny=train_feats[Config.TARGET_NAME].copy()\ntest_X=test_feats[choose_cols].copy()\noof_pred_pro=np.zeros((len(X)))\ntest_pred_pro=np.zeros((Config.num_folds,len(test_X)))\ndel train_feats,test_feats\ngc.collect()\n\nskf = StratifiedKFold(n_splits=Config.num_folds,random_state=Config.seed, shuffle=True)\n\nfor fold, (train_index, valid_index) in (enumerate(skf.split(X, y.astype(str)))):\n    print(f\"fold:{fold}\")\n\n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n\n    model = LinearRegression()\n    model.fit(X_train,y_train)\n    \n    pickle_dump(model, f'/kaggle/working/linear_fold{fold}.model')\n    \n    del model,X_train, X_valid,y_train, y_valid\n    gc.collect()","metadata":{"_uuid":"6ce93277-b869-42e2-ab96-1c0534c7fc9a","_cell_guid":"82b7aeec-ec72-4f7f-8c0a-237e667c914a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}