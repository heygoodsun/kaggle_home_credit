{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"}],"dockerImageVersionId":30665,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# baselineモデル作成","metadata":{}},{"cell_type":"code","source":"import sys\nfrom pathlib import Path\nimport subprocess\nimport os\nimport gc\nfrom glob import glob\n\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom datetime import datetime\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nROOT = '/kaggle/input/home-credit-credit-risk-model-stability'\n\nfrom sklearn.model_selection import TimeSeriesSplit, GroupKFold, StratifiedGroupKFold\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier, Pool\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-24T11:55:48.479945Z","iopub.execute_input":"2024-05-24T11:55:48.480697Z","iopub.status.idle":"2024-05-24T11:55:53.939532Z","shell.execute_reply.started":"2024-05-24T11:55:48.480656Z","shell.execute_reply":"2024-05-24T11:55:53.938248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set a seed for various non-deterministic processes for reproducibility\nimport random\ndef seed_it_all(seed=7):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n\nSEED = 0\n\n# set the seed for this run\nseed_it_all(SEED)","metadata":{"execution":{"iopub.status.busy":"2024-05-24T11:55:53.941017Z","iopub.execute_input":"2024-05-24T11:55:53.942649Z","iopub.status.idle":"2024-05-24T11:55:53.948725Z","shell.execute_reply.started":"2024-05-24T11:55:53.942615Z","shell.execute_reply":"2024-05-24T11:55:53.947873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Pipeline:\n\n    def set_table_dtypes(df):\n        for col in df.columns:\n            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Int64))\n            elif col in [\"date_decision\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n            elif col[-1] in (\"P\", \"A\"):\n                df = df.with_columns(pl.col(col).cast(pl.Float64))\n            elif col[-1] in (\"M\",):\n                df = df.with_columns(pl.col(col).cast(pl.String))\n            elif col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n        return df\n\n    def handle_dates(df):\n        for col in df.columns:\n            if col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))  #!!?\n                df = df.with_columns(pl.col(col).dt.total_days()) # t - t-1\n        df = df.drop(\"date_decision\", \"MONTH\")\n        return df\n\n    def filter_cols(df):\n        for col in df.columns:\n            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n                isnull = df[col].is_null().mean()\n                if isnull > 0.7:\n                    df = df.drop(col)\n        \n        for col in df.columns:\n            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n                freq = df[col].n_unique()\n                if (freq == 1) | (freq > 200):\n                    df = df.drop(col)\n        \n        return df\n\n\nclass Aggregator:\n    #Please add or subtract features yourself, be aware that too many features will take up too much space.\n    def num_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        \n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n        return expr_max +expr_last+expr_mean\n    \n    def date_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"D\")]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n        return  expr_max +expr_last+expr_mean\n    \n    def str_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        #expr_count = [pl.count(col).alias(f\"count_{col}\") for col in cols]\n        return  expr_max +expr_last#+expr_count\n    \n    def other_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        return  expr_max +expr_last\n    \n    def count_expr(df):\n        cols = [col for col in df.columns if \"num_group\" in col]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols] \n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        return  expr_max +expr_last\n    \n    def get_exprs(df):\n        exprs = Aggregator.num_expr(df) + \\\n                Aggregator.date_expr(df) + \\\n                Aggregator.str_expr(df) + \\\n                Aggregator.other_expr(df) + \\\n                Aggregator.count_expr(df)\n\n        return exprs\n\ndef read_file(path, depth=None):\n    df = pl.read_parquet(path)\n    df = df.pipe(Pipeline.set_table_dtypes)\n    if depth in [1,2]:\n        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df)) \n    return df\n\ndef read_files(regex_path, depth=None):\n    chunks = []\n    \n    for path in glob(str(regex_path)):\n        df = pl.read_parquet(path)\n        df = df.pipe(Pipeline.set_table_dtypes)\n        if depth in [1, 2]:\n            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n        chunks.append(df)\n    \n    df = pl.concat(chunks, how=\"vertical_relaxed\")\n    df = df.unique(subset=[\"case_id\"])\n    return df\n\ndef feature_eng(df_base, depth_0, depth_1, depth_2):\n    df_base = (\n        df_base\n        .with_columns(\n            month_decision = pl.col(\"date_decision\").dt.month(),\n            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n        )\n    )\n    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n    df_base = df_base.pipe(Pipeline.handle_dates)\n    return df_base\n\ndef to_pandas(df_data, cat_cols=None):\n    df_data = df_data.to_pandas()\n    if cat_cols is None:\n        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n    return df_data, cat_cols\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        if str(col_type)==\"category\":\n            continue\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            continue\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-05-24T11:55:53.950039Z","iopub.execute_input":"2024-05-24T11:55:53.952123Z","iopub.status.idle":"2024-05-24T11:55:53.999952Z","shell.execute_reply.started":"2024-05-24T11:55:53.952045Z","shell.execute_reply":"2024-05-24T11:55:53.998905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n\nTRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\nTEST_DIR        = ROOT / \"parquet_files\" / \"test\"","metadata":{"execution":{"iopub.status.busy":"2024-05-24T11:55:54.003869Z","iopub.execute_input":"2024-05-24T11:55:54.004518Z","iopub.status.idle":"2024-05-24T11:55:54.018331Z","shell.execute_reply.started":"2024-05-24T11:55:54.004484Z","shell.execute_reply":"2024-05-24T11:55:54.017141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_store = {\n    \"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\"),\n    \"depth_0\": [\n        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n    ],\n    \"depth_1\": [\n        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n    ],\n    \"depth_2\": [\n        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n        read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n        read_file(TRAIN_DIR / \"train_applprev_2.parquet\", 2),\n        read_file(TRAIN_DIR / \"train_person_2.parquet\", 2)\n    ]\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-24T11:55:54.019741Z","iopub.execute_input":"2024-05-24T11:55:54.020770Z","iopub.status.idle":"2024-05-24T11:59:05.556537Z","shell.execute_reply.started":"2024-05-24T11:55:54.020736Z","shell.execute_reply":"2024-05-24T11:59:05.554736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = feature_eng(**data_store)\nprint(\"train data shape:\\t\", df_train.shape)\ndel data_store\ngc.collect()\ndf_train = df_train.pipe(Pipeline.filter_cols)\ndf_train, cat_cols = to_pandas(df_train)\ndf_train = reduce_mem_usage(df_train)","metadata":{"execution":{"iopub.status.busy":"2024-05-24T11:59:05.559856Z","iopub.execute_input":"2024-05-24T11:59:05.560397Z","iopub.status.idle":"2024-05-24T12:00:26.233963Z","shell.execute_reply.started":"2024-05-24T11:59:05.560346Z","shell.execute_reply":"2024-05-24T12:00:26.232595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DEBUGモードの場合、データの件数を制限\nif DEBUG_MODE:\n    df_train = df_train.iloc[:5000]\n    \nprint(\"train data shape:\\t\", df_train.shape)\nnums=df_train.select_dtypes(exclude='category').columns\nfrom itertools import combinations, permutations\n#df_train=df_train[nums]\nnans_df = df_train[nums].isna()\nnans_groups={}\nfor col in nums:\n    cur_group = nans_df[col].sum()\n    try:\n        nans_groups[cur_group].append(col)\n    except:\n        nans_groups[cur_group]=[col]\ndel nans_df; x=gc.collect()\n\ndef reduce_group(grps):\n    use = []\n    for g in grps:\n        mx = 0; vx = g[0]\n        for gg in g:\n            n = df_train[gg].nunique()\n            if n>mx:\n                mx = n\n                vx = gg\n            #print(str(gg)+'-'+str(n),', ',end='')\n        use.append(vx)\n        #print()\n    print('Use these',use)\n    return use\n\ndef group_columns_by_correlation(matrix, threshold=0.8):\n    # 计算列之间的相关性\n    correlation_matrix = matrix.corr()\n\n    # 分组列\n    groups = []\n    remaining_cols = list(matrix.columns)\n    while remaining_cols:\n        col = remaining_cols.pop(0)\n        group = [col]\n        correlated_cols = [col]\n        for c in remaining_cols:\n            if correlation_matrix.loc[col, c] >= threshold:\n                group.append(c)\n                correlated_cols.append(c)\n        groups.append(group)\n        remaining_cols = [c for c in remaining_cols if c not in correlated_cols]\n    \n    return groups\n\nuses=[]\nfor k,v in nans_groups.items():\n    if len(v)>1:\n            Vs = nans_groups[k]\n            #cross_features=list(combinations(Vs, 2))\n            #make_corr(Vs)\n            grps= group_columns_by_correlation(df_train[Vs], threshold=0.8)\n            use=reduce_group(grps)\n            uses=uses+use\n            #make_corr(use)\n    else:\n        uses=uses+v\n    print('####### NAN count =',k)\nprint(uses)\nprint(len(uses))\nuses=uses+list(df_train.select_dtypes(include='category').columns)\n\n# WEEK_NUMが除外されてたら追加する\nif 'WEEK_NUM' not in uses:\n    uses.append('WEEK_NUM')\n    \nprint(len(uses))\ndf_train=df_train[uses]","metadata":{"execution":{"iopub.status.busy":"2024-05-24T12:00:26.235993Z","iopub.execute_input":"2024-05-24T12:00:26.236397Z","iopub.status.idle":"2024-05-24T12:00:26.868105Z","shell.execute_reply.started":"2024-05-24T12:00:26.236364Z","shell.execute_reply":"2024-05-24T12:00:26.866964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 訓練データのカラム情報を出力\nimport joblib\njoblib.dump((df_train.columns, cat_cols), \"train_columns.pkl\")","metadata":{"execution":{"iopub.status.busy":"2024-05-24T12:00:26.869280Z","iopub.execute_input":"2024-05-24T12:00:26.869615Z","iopub.status.idle":"2024-05-24T12:00:26.881058Z","shell.execute_reply.started":"2024-05-24T12:00:26.869588Z","shell.execute_reply":"2024-05-24T12:00:26.879686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample = pd.read_csv(\"/kaggle/input/home-credit-credit-risk-model-stability/sample_submission.csv\")\n# device='gpu'\n# n_est=6000\n# DRY_RUN = True if sample.shape[0] == 10 else False   \n# if DRY_RUN:\n#     device='cpu'\n#     df_train = df_train.iloc[:50000]\n#     n_est=600","metadata":{"execution":{"iopub.status.busy":"2024-05-24T12:00:26.882931Z","iopub.execute_input":"2024-05-24T12:00:26.883350Z","iopub.status.idle":"2024-05-24T12:00:26.888515Z","shell.execute_reply.started":"2024-05-24T12:00:26.883314Z","shell.execute_reply":"2024-05-24T12:00:26.887316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_store = {\n    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\"),\n    \"depth_0\": [\n        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n    ],\n    \"depth_1\": [\n        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n    ],\n    \"depth_2\": [\n        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n        read_file(TEST_DIR / \"test_applprev_2.parquet\", 2),\n        read_file(TEST_DIR / \"test_person_2.parquet\", 2)\n    ]\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-24T12:00:26.890471Z","iopub.execute_input":"2024-05-24T12:00:26.890898Z","iopub.status.idle":"2024-05-24T12:00:27.296749Z","shell.execute_reply.started":"2024-05-24T12:00:26.890861Z","shell.execute_reply":"2024-05-24T12:00:27.295838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = feature_eng(**data_store)\nprint(\"test data shape:\\t\", df_test.shape)\ndel data_store\ngc.collect()\ndf_test = df_test.select([col for col in df_train.columns if col != \"target\"])\nprint(\"train data shape:\\t\", df_train.shape)\nprint(\"test data shape:\\t\", df_test.shape)\n\ndf_test, cat_cols = to_pandas(df_test, cat_cols)\ndf_test = reduce_mem_usage(df_test)\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-05-24T12:00:27.299424Z","iopub.execute_input":"2024-05-24T12:00:27.299829Z","iopub.status.idle":"2024-05-24T12:00:27.871772Z","shell.execute_reply.started":"2024-05-24T12:00:27.299791Z","shell.execute_reply":"2024-05-24T12:00:27.870463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Selection","metadata":{}},{"cell_type":"code","source":"y = df_train[\"target\"]\nweeks = df_train[\"WEEK_NUM\"]\ndf_train= df_train.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])\ncv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=SEED)","metadata":{"execution":{"iopub.status.busy":"2024-05-24T12:00:27.873400Z","iopub.execute_input":"2024-05-24T12:00:27.873854Z","iopub.status.idle":"2024-05-24T12:00:27.953275Z","shell.execute_reply.started":"2024-05-24T12:00:27.873815Z","shell.execute_reply":"2024-05-24T12:00:27.951914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[cat_cols] = df_train[cat_cols].astype(str)\ndf_test[cat_cols] = df_test[cat_cols].astype(str)","metadata":{"execution":{"iopub.status.busy":"2024-05-24T12:00:27.958358Z","iopub.execute_input":"2024-05-24T12:00:27.958748Z","iopub.status.idle":"2024-05-24T12:00:28.051355Z","shell.execute_reply.started":"2024-05-24T12:00:27.958717Z","shell.execute_reply":"2024-05-24T12:00:28.050281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 学習","metadata":{}},{"cell_type":"code","source":"if DEBUG_MODE:\n    task_type='CPU'\n    device='cpu'\n    n_est=100\nelse:\n    task_type='GPU'\n    device='gpu'\n    n_est=6000","metadata":{"execution":{"iopub.status.busy":"2024-05-24T12:09:47.728935Z","iopub.execute_input":"2024-05-24T12:09:47.729352Z","iopub.status.idle":"2024-05-24T12:09:47.735589Z","shell.execute_reply.started":"2024-05-24T12:09:47.729318Z","shell.execute_reply":"2024-05-24T12:09:47.734032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LGBMはアルゴリズムが異なる2つのモデルを学習\nparams_lgb = {\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",\n    \"max_depth\": 10,  \n    \"learning_rate\": 0.05,\n    \"n_estimators\": 2500,  \n    \"colsample_bytree\": 0.8,\n    \"colsample_bynode\": 0.8,\n    \"verbose\": -1,\n    \"random_state\": SEED,\n    \"reg_alpha\": 0.1,\n    \"reg_lambda\": 10,\n    \"extra_trees\":True,\n    'num_leaves':64,\n    \"device\": device, \n}","metadata":{"execution":{"iopub.status.busy":"2024-05-24T12:09:47.885734Z","iopub.execute_input":"2024-05-24T12:09:47.886142Z","iopub.status.idle":"2024-05-24T12:09:47.892695Z","shell.execute_reply.started":"2024-05-24T12:09:47.886109Z","shell.execute_reply":"2024-05-24T12:09:47.891382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_lgb2 = {\n    \"boosting_type\": \"goss\",\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",\n    \"max_depth\": 10,  \n    \"learning_rate\": 0.05,\n    \"n_estimators\": 2500,  \n    \"colsample_bytree\": 0.8,\n    \"colsample_bynode\": 0.8,\n    \"verbose\": -1,\n    \"random_state\": SEED,\n    \"reg_alpha\": 0.1,\n    \"reg_lambda\": 10,\n    \"extra_trees\":True,\n    'num_leaves':64,\n    \"device\": device, \n}","metadata":{"execution":{"iopub.status.busy":"2024-05-24T12:09:48.170340Z","iopub.execute_input":"2024-05-24T12:09:48.170737Z","iopub.status.idle":"2024-05-24T12:09:48.177467Z","shell.execute_reply.started":"2024-05-24T12:09:48.170708Z","shell.execute_reply":"2024-05-24T12:09:48.176084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fitted_models_cb = []\nfitted_models_lgb = []\nfitted_models_lgb2 = []\nfitted_models_eclf = []\ncv_scores_cb = []\ncv_scores_lgb = []\ncv_scores_lgb2 = []\ncv_scores_eclf = []\n\nfor idx_train, idx_valid in cv.split(df_train, y, groups=weeks):#\n    X_train, y_train = df_train.iloc[idx_train], y.iloc[idx_train]# \n    X_valid, y_valid = df_train.iloc[idx_valid], y.iloc[idx_valid]\n    \n    # CatBoost\n    train_pool = Pool(X_train, y_train, cat_features=cat_cols)\n    val_pool = Pool(X_valid, y_valid, cat_features=cat_cols)\n    clf_cb = CatBoostClassifier(\n        eval_metric='AUC',\n        task_type=task_type,\n        learning_rate=0.05,\n        iterations=n_est)\n    random_seed=SEED\n    clf_cb.fit(train_pool, eval_set=val_pool,verbose=300)\n    fitted_models_cb.append(clf_cb)\n    y_pred_valid = clf_cb.predict_proba(X_valid)[:,1]\n#     auc_score = roc_auc_score(y_valid, y_pred_valid)\n#     cv_scores_cb.append(auc_score)\n    \n    # LGBM用データ変換\n    X_train[cat_cols] = X_train[cat_cols].astype(\"category\")\n    X_valid[cat_cols] = X_valid[cat_cols].astype(\"category\")\n    \n    # LGBM1\n    clf_lgb = LGBMClassifier(**params_lgb)\n    clf_lgb.fit(\n        X_train, y_train,\n        eval_set = [(X_valid, y_valid)],\n        callbacks = [lgb.log_evaluation(200), lgb.early_stopping(60)] )\n    \n    fitted_models_lgb.append(clf_lgb)\n    y_pred_valid = clf_lgb.predict_proba(X_valid)[:,1]\n#     auc_score = roc_auc_score(y_valid, y_pred_valid)\n#     cv_scores_lgb.append(auc_score)\n\n    # LGBM2\n    clf_lgb2 = LGBMClassifier(**params_lgb2)\n    clf_lgb2.fit(\n        X_train, y_train,\n        eval_set = [(X_valid, y_valid)],\n        callbacks = [lgb.log_evaluation(200), lgb.early_stopping(60)] )\n    \n    fitted_models_lgb2.append(clf_lgb2)\n    y_pred_valid = clf_lgb2.predict_proba(X_valid)[:,1]\n#     auc_score = roc_auc_score(y_valid, y_pred_valid)\n#     cv_scores_lgb2.append(auc_score)\n \n    # 2つのLGBMをアンサンブル\n    eclf = VotingClassifier(\n     estimators=[('lgb', clf_lgb), ('lgb2', clf_lgb2)],\n     voting='soft', weights=[1, 1])   # 1:1の重みで、2つのモデルの予測値の平均を取る\n    eclf = eclf.fit(X_train, y_train)\n    fitted_models_eclf.append(eclf)\n    y_pred_valid = eclf.predict_proba(X_valid)[:,1]\n#     auc_score = roc_auc_score(y_valid, y_pred_valid)\n#     cv_scores_eclf.append(auc_score)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-24T12:13:44.812469Z","iopub.execute_input":"2024-05-24T12:13:44.816762Z","iopub.status.idle":"2024-05-24T12:17:02.766835Z","shell.execute_reply.started":"2024-05-24T12:13:44.816693Z","shell.execute_reply":"2024-05-24T12:17:02.765638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"CatBoost\")   \n# print(\"CV AUC scores: \", cv_scores_cb)\n# print(\"Maximum CV AUC score: \", max(cv_scores_cb))\n# print(\"LightGBM\")\n# print(\"CV AUC scores: \", cv_scores_lgb)\n# print(\"Maximum CV AUC score: \", max(cv_scores_lgb))\n# print(\"LightGBM_goss\")\n# print(\"CV AUC scores: \", cv_scores_lgb2)\n# print(\"Maximum CV AUC score: \", max(cv_scores_lgb2))\n# print(\"Ensemble of LGBM and LGBM_goss\")\n# print(\"CV AUC scores: \", cv_scores_eclf)\n# print(\"Maximum CV AUC score: \", max(cv_scores_eclf))","metadata":{"execution":{"iopub.status.busy":"2024-05-24T12:12:51.282352Z","iopub.status.idle":"2024-05-24T12:12:51.282887Z","shell.execute_reply.started":"2024-05-24T12:12:51.282659Z","shell.execute_reply":"2024-05-24T12:12:51.282680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## モデルの保存","metadata":{}},{"cell_type":"code","source":"import dill\ndef pickle_dump(obj, path):\n   # 指定のpathをバイナリ書き込みモードで開く\n   with open(path, mode=\"wb\") as f:\n       # objをプロトコルバージョン4を使用してシリアライズし、fに書き込む\n       dill.dump(obj, f, protocol=4)\n\n# CatBoostとLGBMのアンサンブルしたやつを保存\nfor i, model in enumerate(fitted_models_cb):\n    pickle_dump(model, f'/kaggle/working/cat_fold{i}.model')\nfor i, model in enumerate(fitted_models_eclf):\n    pickle_dump(model, f'/kaggle/working/eclf_fold{i}.model')","metadata":{"execution":{"iopub.status.busy":"2024-05-24T12:18:05.062289Z","iopub.execute_input":"2024-05-24T12:18:05.064401Z","iopub.status.idle":"2024-05-24T12:18:11.059055Z","shell.execute_reply.started":"2024-05-24T12:18:05.064328Z","shell.execute_reply":"2024-05-24T12:18:11.057403Z"},"trusted":true},"execution_count":null,"outputs":[]}]}